import random
import numpy as np
from typing import List, Callable
import torch
from transformers import MarianMTModel, MarianTokenizer
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize
import nltk
from config_gpu import config
from utils_gpu import cleanup_gpu_memory

# –ó–∞–≥—Ä—É–∂–∞–µ–º NLTK —Ä–µ—Å—É—Ä—Å—ã
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')


class GPUTransformations:
    """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è Stability Score —Å GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º."""

    def __init__(self):
        self.device = config.DEVICE

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –¥–ª—è back-translation –Ω–∞ GPU
        print("üåç –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏ –¥–ª—è Stability Score...")

        try:
            # –ú–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ English -> German
            self.en_de_tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')
            self.en_de_model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-de').to(self.device)
            self.en_de_model.eval()

            # –ú–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ German -> English
            self.de_en_tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-de-en')
            self.de_en_model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-de-en').to(self.device)
            self.de_en_model.eval()

            self.has_translation = True
            print(f"‚úÖ –ú–æ–¥–µ–ª–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ {self.device}")
        except Exception as e:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏: {e}")
            self.has_translation = False

    def back_translation_gpu(self, text: str) -> str:
        """–û–±—Ä–∞—Ç–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ GPU (English -> German -> English)."""
        if not self.has_translation:
            return text

        try:
            # English -> German
            batch = self.en_de_tokenizer([text], return_tensors="pt", padding=True, truncation=True)
            batch = {k: v.to(self.device) for k, v in batch.items()}

            with torch.no_grad():
                if config.USE_AMP:
                    with torch.cuda.amp.autocast():
                        translated = self.en_de_model.generate(**batch)
                else:
                    translated = self.en_de_model.generate(**batch)

            german_text = self.en_de_tokenizer.decode(translated[0], skip_special_tokens=True)

            # German -> English
            batch = self.de_en_tokenizer([german_text], return_tensors="pt", padding=True, truncation=True)
            batch = {k: v.to(self.device) for k, v in batch.items()}

            with torch.no_grad():
                if config.USE_AMP:
                    with torch.cuda.amp.autocast():
                        back_translated = self.de_en_model.generate(**batch)
                else:
                    back_translated = self.de_en_model.generate(**batch)

            result = self.de_en_tokenizer.decode(back_translated[0], skip_special_tokens=True)

            return result

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ back-translation: {e}")
            return text

    def synonym_replacement(self, text: str, replace_ratio: float = 0.1) -> str:
        """–ó–∞–º–µ–Ω–∞ —Å–ª–æ–≤ –Ω–∞ —Å–∏–Ω–æ–Ω–∏–º—ã."""
        words = word_tokenize(text)
        if len(words) <= 1:
            return text

        n_replace = max(1, int(len(words) * replace_ratio))
        indices_to_replace = random.sample(range(len(words)), n_replace)

        new_words = words.copy()

        for idx in indices_to_replace:
            word = words[idx]

            # –ü–æ–ª—É—á–∞–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã
            synonyms = []
            for syn in wordnet.synsets(word):
                for lemma in syn.lemmas():
                    synonym = lemma.name().replace('_', ' ').lower()
                    if synonym != word.lower():
                        synonyms.append(synonym)

            if synonyms:
                # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —Å–∏–Ω–æ–Ω–∏–º
                new_word = random.choice(synonyms)
                new_words[idx] = new_word

        return ' '.join(new_words)

    def random_deletion(self, text: str, delete_ratio: float = 0.1) -> str:
        """–°–ª—É—á–∞–π–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —Å–ª–æ–≤."""
        words = text.split()
        if len(words) <= 1:
            return text

        n_delete = max(1, int(len(words) * delete_ratio))
        indices_to_keep = sorted(random.sample(range(len(words)), len(words) - n_delete))

        new_words = [words[i] for i in indices_to_keep]
        return ' '.join(new_words)

    def random_swap(self, text: str, swap_ratio: float = 0.1) -> str:
        """–°–ª—É—á–∞–π–Ω–∞—è –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–ª–æ–≤."""
        words = text.split()
        if len(words) <= 1:
            return text

        n_swap = max(1, int(len(words) * swap_ratio))

        for _ in range(n_swap):
            idx1, idx2 = random.sample(range(len(words)), 2)
            words[idx1], words[idx2] = words[idx2], words[idx1]

        return ' '.join(words)

    def apply_transformations(self, text: str, n_transformations: int = None) -> List[str]:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π –∫ —Ç–µ–∫—Å—Ç—É.

        Returns:
            –°–ø–∏—Å–æ–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        """
        if n_transformations is None:
            n_transformations = config.N_TRANSFORMATIONS

        transformations = []

        # –°–æ–±–∏—Ä–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
        available_transforms = []

        if self.has_translation:
            available_transforms.append(('back_translation', self.back_translation_gpu))

        available_transforms.extend([
            ('synonym_replacement', lambda t: self.synonym_replacement(t, 0.1)),
            ('random_deletion', lambda t: self.random_deletion(t, 0.1)),
            ('random_swap', lambda t: self.random_swap(t, 0.1))
        ])

        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
        selected_transforms = random.sample(
            available_transforms,
            min(n_transformations, len(available_transforms))
        )

        for name, transform in selected_transforms:
            try:
                transformed = transform(text)
                if transformed != text and len(transformed) > 10:
                    transformations.append(transformed)
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ {name}: {e}")

        return transformations


# –°–∏–Ω–≥–ª—Ç–æ–Ω –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä–∞
_transformations = None


def get_transformations():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–æ—Ä–∞."""
    global _transformations
    if _transformations is None:
        _transformations = GPUTransformations()
    return _transformations