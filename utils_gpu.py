import pandas as pd
import numpy as np
import re
from typing import List, Tuple, Dict, Any, Optional
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk import pos_tag
from nltk.corpus import stopwords
import torch
from tqdm import tqdm
import joblib
import os
from pathlib import Path
from config_gpu import config


# –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ NLTK
def setup_nltk():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã NLTK."""
    try:
        nltk.data.find('tokenizers/punkt_tab')
    except LookupError:
        nltk.download('punkt')
        nltk.download('averaged_perceptron_tagger')
        nltk.download('stopwords')
        nltk.download('wordnet')
    return True


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º NLTK
setup_nltk()


def load_data_gpu(file_path: Path, max_samples: Optional[int] = None) -> Tuple[List[str], List[int]]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è GPU.

    Args:
        file_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
        max_samples: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)

    Returns:
        texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
        labels: —Å–ø–∏—Å–æ–∫ –º–µ—Ç–æ–∫
    """
    if not file_path.exists():
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞
    if file_path.suffix.lower() == '.csv':
        if max_samples:
            df = pd.read_csv(file_path, nrows=max_samples)
        else:
            df = pd.read_csv(file_path)
    else:
        if max_samples:
            df = pd.read_excel(file_path, nrows=max_samples)
        else:
            df = pd.read_excel(file_path)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–æ–Ω–∫–∏
    required_columns = ['text', 'label']
    for col in required_columns:
        if col not in df.columns:
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
            alt_names = {
                'text': ['content', 'sentence', 'document', 'Text', 'Content'],
                'label': ['target', 'is_ai', 'ai_generated', 'Label', 'Target']
            }

            found = False
            for alt in alt_names[col]:
                if alt in df.columns:
                    df = df.rename(columns={alt: col})
                    found = True
                    break

            if not found:
                raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ '{col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")

    # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    df = df.dropna(subset=['text', 'label'])

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∫–∏ –≤ int
    df['label'] = df['label'].astype(int)

    # –ë–µ—Ä–µ–º –≤—ã–±–æ—Ä–∫—É –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if max_samples and len(df) > max_samples:
        df = df.sample(max_samples, random_state=42)

    texts = df['text'].astype(str).tolist()
    labels = df['label'].astype(int).tolist()

    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ {file_path.name}")
    print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: Human (0): {labels.count(0)}, AI (1): {labels.count(1)}")

    return texts, labels


def extract_stylometric_features_gpu(text: str) -> Dict[str, float]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π.

    Args:
        text: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    """
    if not text or len(text.strip()) == 0:
        return {feature: 0.0 for feature in config.STYLOMETRIC_FEATURES}

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    sentences = sent_tokenize(text)
    words = word_tokenize(text.lower())

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
    words_alpha = [w for w in words if w.isalpha()]

    if len(words_alpha) == 0:
        return {feature: 0.0 for feature in config.STYLOMETRIC_FEATURES}

    # –ß–∞—Å—Ç–∏ —Ä–µ—á–∏
    pos_tags = pos_tag(words_alpha)

    # –°—á–µ—Ç—á–∏–∫–∏ POS
    pos_counts = {}
    for _, tag in pos_tags:
        pos_counts[tag] = pos_counts.get(tag, 0) + 1

    # –°–ø–∏—Å–∫–∏ —Å—Ç–æ–ø-—Å–ª–æ–≤
    stop_words = set(stopwords.words('english'))
    function_words = [w for w in words_alpha if w in stop_words]
    content_words = [w for w in words_alpha if w not in stop_words]

    # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
    features = {}

    # –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    features['text_length'] = len(text)
    features['num_sentences'] = len(sentences)
    features['num_words'] = len(words_alpha)
    features['num_unique_words'] = len(set(words_alpha))

    # –õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
    features['ttr'] = features['num_unique_words'] / features['num_words'] if features['num_words'] > 0 else 0

    # –î–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    sent_lengths = [len(sent.split()) for sent in sentences if len(sent.split()) > 0]
    features['avg_sentence_length'] = np.mean(sent_lengths) if sent_lengths else 0
    features['std_sentence_length'] = np.std(sent_lengths) if len(sent_lengths) > 1 else 0

    # –î–ª–∏–Ω–∞ —Å–ª–æ–≤
    word_lengths = [len(w) for w in words_alpha]
    features['avg_word_length'] = np.mean(word_lengths) if word_lengths else 0

    # –ß–∞—Å—Ç–æ—Ç–∞ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    features['comma_freq'] = text.count(',') / len(text) if len(text) > 0 else 0
    features['period_freq'] = text.count('.') / len(text) if len(text) > 0 else 0
    features['question_freq'] = text.count('?') / len(text) if len(text) > 0 else 0
    features['exclamation_freq'] = text.count('!') / len(text) if len(text) > 0 else 0

    # –ß–∞—Å—Ç–∏ —Ä–µ—á–∏
    total_pos = sum(pos_counts.values()) if pos_counts else 1
    features['noun_ratio'] = sum(pos_counts.get(tag, 0) for tag in pos_counts if tag.startswith('NN')) / total_pos
    features['verb_ratio'] = sum(pos_counts.get(tag, 0) for tag in pos_counts if tag.startswith('VB')) / total_pos
    features['adjective_ratio'] = sum(pos_counts.get(tag, 0) for tag in pos_counts if tag.startswith('JJ')) / total_pos
    features['adverb_ratio'] = sum(pos_counts.get(tag, 0) for tag in pos_counts if tag.startswith('RB')) / total_pos
    features['pronoun_ratio'] = sum(pos_counts.get(tag, 0) for tag in pos_counts if tag.startswith('PR')) / total_pos

    # –°—Ç–æ–ø-—Å–ª–æ–≤–∞
    features['function_word_ratio'] = len(function_words) / len(words_alpha) if words_alpha else 0
    features['content_word_ratio'] = len(content_words) / len(words_alpha) if words_alpha else 0

    # –î—Ä—É–≥–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    features['capital_ratio'] = sum(1 for c in text if c.isupper()) / len(text) if len(text) > 0 else 0
    features['digit_ratio'] = sum(1 for c in text if c.isdigit()) / len(text) if len(text) > 0 else 0

    return features


def batch_extract_stylometric_features(texts: List[str]) -> np.ndarray:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤.

    Args:
        texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤

    Returns:
        –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    """
    features_list = []

    for text in tqdm(texts, desc="–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"):
        features = extract_stylometric_features_gpu(text)
        feature_vector = [features[feat] for feat in config.STYLOMETRIC_FEATURES]
        features_list.append(feature_vector)

    return np.array(features_list)


def save_gpu_features(features_dict: Dict[str, Any], name: str):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Ñ–∞–π–ª."""
    path = config.FEATURES_DIR / f"{name}.pkl"
    joblib.dump(features_dict, path, compress=3)
    print(f"üíæ –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {path}")


def load_gpu_features(name: str) -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Ñ–∞–π–ª–∞."""
    path = config.FEATURES_DIR / f"{name}.pkl"
    return joblib.load(path)


def cleanup_gpu_memory():
    """–û—á–∏—â–∞–µ—Ç –ø–∞–º—è—Ç—å GPU."""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        import gc
        gc.collect()