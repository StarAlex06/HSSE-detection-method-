import torch
import numpy as np
from transformers import GPT2LMHeadModel, GPT2TokenizerFast
from transformers import BertForMaskedLM, BertTokenizerFast
from tqdm import tqdm
from config_gpu import config
from utils_gpu import cleanup_gpu_memory


class GPUPerplexityCalculator:
    """–í—ã—á–∏—Å–ª—è–µ—Ç –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é –Ω–∞ GPU –¥–ª—è HSSE –º–µ—Ç–æ–¥–∞."""

    def __init__(self):
        self.device = config.DEVICE

        print("ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏ –Ω–∞ GPU...")

        # –ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (GPT-2)
        print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ {config.AR_MODEL_NAME}...")
        self.ar_tokenizer = GPT2TokenizerFast.from_pretrained(config.AR_MODEL_NAME)
        self.ar_model = GPT2LMHeadModel.from_pretrained(config.AR_MODEL_NAME).to(self.device)
        self.ar_model.eval()

        # –ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (BERT)
        print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ {config.MLM_MODEL_NAME}...")
        self.mlm_tokenizer = BertTokenizerFast.from_pretrained(config.MLM_MODEL_NAME)
        self.mlm_model = BertForMaskedLM.from_pretrained(config.MLM_MODEL_NAME).to(self.device)
        self.mlm_model.eval()

        print(f"‚úÖ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ {self.device}")

    def calculate_perplexity_ar(self, text: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é —Å –ø–æ–º–æ—â—å—é –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏."""
        try:
            encodings = self.ar_tokenizer(text, return_tensors='pt')
            input_ids = encodings.input_ids.to(self.device)

            with torch.no_grad():
                if config.USE_AMP:
                    with torch.cuda.amp.autocast():
                        outputs = self.ar_model(input_ids, labels=input_ids)
                else:
                    outputs = self.ar_model(input_ids, labels=input_ids)

            loss = outputs.loss
            perplexity = torch.exp(loss).cpu().item()

            return perplexity
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ AR perplexity: {e}")
            return 0.0

    def calculate_perplexity_mlm(self, text: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é —Å –ø–æ–º–æ—â—å—é –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏."""
        try:
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
            tokens = self.mlm_tokenizer.tokenize(text)
            if len(tokens) > 510:
                tokens = tokens[:510]

            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
            tokens = ['[CLS]'] + tokens + ['[SEP]']
            input_ids = self.mlm_tokenizer.convert_tokens_to_ids(tokens)
            input_tensor = torch.tensor([input_ids]).to(self.device)

            with torch.no_grad():
                if config.USE_AMP:
                    with torch.cuda.amp.autocast():
                        outputs = self.mlm_model(input_tensor)
                else:
                    outputs = self.mlm_model(input_tensor)

                logits = outputs.logits

            # –í—ã—á–∏—Å–ª—è–µ–º perplexity
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = input_tensor[..., 1:].contiguous()

            loss_fct = torch.nn.CrossEntropyLoss(reduction='mean')
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),
                            shift_labels.view(-1))

            perplexity = torch.exp(loss).cpu().item()

            return perplexity
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ MLM perplexity: {e}")
            return 0.0

    def calculate_perplexity_gap(self, text: str) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç Perplexity Gap –¥–ª—è HSSE –º–µ—Ç–æ–¥–∞.

        Œî_ppl(x) = log(PPL_AR(x)) - log(PPL_MLM(x))
        """
        ppl_ar = self.calculate_perplexity_ar(text)
        ppl_mlm = self.calculate_perplexity_mlm(text)

        # –ó–∞—â–∏—Ç–∞ –æ—Ç –Ω—É–ª–µ–π
        if ppl_ar == 0 or ppl_mlm == 0:
            return 0.0

        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω–∏—Ü—É –≤ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–π —à–∫–∞–ª–µ
        gap = np.log(ppl_ar) - np.log(ppl_mlm)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º (—ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞–Ω–∏—Ü—ã)
        gap = np.clip(gap, -5, 5)

        return float(gap)

    def batch_calculate_perplexity_gap(self, texts: list, batch_size: int = None) -> np.ndarray:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç Perplexity Gap –¥–ª—è –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤.

        Args:
            texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (None = –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π)

        Returns:
            –ú–∞—Å—Å–∏–≤ —Å Perplexity Gap –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        """
        if batch_size is None:
            batch_size = config.PERPLEXITY_BATCH_SIZE

        gaps = []

        print(f"üìä –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Perplexity Gap –¥–ª—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤...")

        for i in tqdm(range(0, len(texts), batch_size), desc="Perplexity Gap"):
            batch_texts = texts[i:i + batch_size]

            for text in batch_texts:
                try:
                    gap = self.calculate_perplexity_gap(str(text))
                    gaps.append(gap)
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ {i}: {e}")
                    gaps.append(0.0)

            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞
            cleanup_gpu_memory()

        return np.array(gaps)


# –°–∏–Ω–≥–ª—Ç–æ–Ω –¥–ª—è –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä–∞
_perplexity_calculator = None


def get_perplexity_calculator():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä–∞ –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏."""
    global _perplexity_calculator
    if _perplexity_calculator is None:
        _perplexity_calculator = GPUPerplexityCalculator()
    return _perplexity_calculator