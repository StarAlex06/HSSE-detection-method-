import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
import joblib
from typing import List, Tuple, Dict, Any
from transformers import AutoTokenizer, AutoModelForSequenceClassification

from config_gpu import config
from utils_gpu import load_data_gpu, extract_stylometric_features_gpu, save_gpu_features
from perplexity_gpu import get_perplexity_calculator
from transformations_gpu import get_transformations


class SemanticModelGPU:
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ GPU."""

    def __init__(self, model_path):
        self.device = config.DEVICE
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path).to(self.device)
        self.model.eval()

    def predict_proba(self, text: str) -> float:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç–æ–≥–æ, —á—Ç–æ —Ç–µ–∫—Å—Ç AI-generated."""
        encoding = self.tokenizer(
            text,
            max_length=config.MAX_LENGTH,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        with torch.no_grad():
            input_ids = encoding['input_ids'].to(self.device)
            attention_mask = encoding['attention_mask'].to(self.device)

            if config.USE_AMP:
                with torch.cuda.amp.autocast():
                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
            else:
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)

            probs = torch.softmax(outputs.logits, dim=1)

        return probs[0, 1].item()


class HSSEFeatureExtractorGPU:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ 4 –ø—Ä–∏–∑–Ω–∞–∫–∞ HSSE –º–µ—Ç–æ–¥–∞ –Ω–∞ GPU."""

    def __init__(self):
        print("=" * 70)
        print("üîç HSSE - –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í –ù–ê GPU")
        print("=" * 70)

        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –º–æ–¥–µ–ª—å
        print("\n1Ô∏è‚É£  –ó–∞–≥—Ä—É–∑–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏...")
        self.semantic_model = SemanticModelGPU(config.SEMANTIC_MODEL_PATH)

        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é –º–æ–¥–µ–ª—å
        print("2Ô∏è‚É£  –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏...")
        self.stylometric_model = joblib.load(config.STYLOMETRIC_MODEL_PATH)
        self.stylometric_scaler = joblib.load(config.STYLOMETRIC_SCALER_PATH)

        # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏
        print("3Ô∏è‚É£  –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä–∞ –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏...")
        self.perplexity_calc = get_perplexity_calculator()

        # 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
        print("4Ô∏è‚É£  –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π...")
        self.transformator = get_transformations()

        print("‚úÖ HSSE Feature Extractor –≥–æ—Ç–æ–≤!")

    def extract_single(self, text: str) -> np.ndarray:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ 4 –ø—Ä–∏–∑–Ω–∞–∫–∞ HSSE –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.

        Returns:
            [semantic_score, stylometric_score, perplexity_gap, stability_score]
        """
        if not text or len(text.strip()) < 10:
            return np.zeros(4, dtype=np.float32)

        features = []

        # 1. Semantic Score (p_sem)
        try:
            p_sem = self.semantic_model.predict_proba(text)
            features.append(p_sem)
        except:
            features.append(0.5)

        # 2. Stylometric Score (p_sty)
        try:
            style_features = extract_stylometric_features_gpu(text)
            style_vector = np.array([style_features[feat] for feat in config.STYLOMETRIC_FEATURES]).reshape(1, -1)
            style_vector_scaled = self.stylometric_scaler.transform(style_vector)
            p_sty = self.stylometric_model.predict_proba(style_vector_scaled)[0, 1]
            features.append(p_sty)
        except:
            features.append(0.5)

        # 3. Perplexity Gap (Œî_ppl)
        try:
            delta_ppl = self.perplexity_calc.calculate_perplexity_gap(text)
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É [-0.5, 0.5]
            delta_ppl_norm = np.clip(delta_ppl, -5, 5) / 10.0
            features.append(delta_ppl_norm)
        except:
            features.append(0.0)

        # 4. Stability Score (s)
        try:
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
            transformed_texts = self.transformator.apply_transformations(
                text,
                config.N_TRANSFORMATIONS
            )

            # –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
            all_probs = [features[0]]  # –ù–∞—á–∏–Ω–∞–µ–º —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏

            for transformed_text in transformed_texts:
                try:
                    prob = self.semantic_model.predict_proba(transformed_text)
                    all_probs.append(prob)
                except:
                    continue

            # –í—ã—á–∏—Å–ª—è–µ–º –¥–∏—Å–ø–µ—Ä—Å–∏—é –∫–∞–∫ –º–µ—Ä—É –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            if len(all_probs) > 1:
                stability = np.var(all_probs)
            else:
                stability = 0.0

            features.append(stability)

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ Stability Score: {e}")
            features.append(0.0)

        return np.array(features, dtype=np.float32)

    def extract_batch(self, texts: List[str]) -> Tuple[np.ndarray, List[Dict[str, Any]]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ HSSE –¥–ª—è –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤.

        Returns:
            features_array: –º–∞—Å—Å–∏–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ [n_samples, 4]
            raw_features: —Å–ø–∏—Å–æ–∫ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        """
        features_list = []
        raw_features_list = []

        print(f"üìä –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤...")

        for i, text in enumerate(tqdm(texts, desc="HSSE Features")):
            try:
                features = self.extract_single(str(text))
                features_list.append(features)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                raw_features = {
                    'text': text[:100] + '...' if len(text) > 100 else text,
                    'p_sem': features[0],
                    'p_sty': features[1],
                    'delta_ppl': features[2],
                    'stability': features[3],
                    'features': features.tolist()
                }
                raw_features_list.append(raw_features)

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ {i}: {e}")
                features_list.append(np.zeros(4, dtype=np.float32))

        return np.array(features_list), raw_features_list


def extract_hsse_features():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ HSSE."""
    extractor = HSSEFeatureExtractorGPU()

    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    datasets = {
        'train': config.TRAIN_PATH,
        'val': config.VAL_PATH,
        'test': config.TEST_PATH
    }

    for name, path in datasets.items():
        print(f"\n{'=' * 60}")
        print(f"üìÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ {name} –¥–∞—Ç–∞—Å–µ—Ç–∞")
        print('=' * 60)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        texts, labels = load_data_gpu(path)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        features, raw_features = extractor.extract_batch(texts)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        features_dict = {
            'X': features,
            'y': np.array(labels),
            'texts': texts,
            'raw_features': raw_features,
            'feature_names': ['semantic_score', 'stylometric_score', 'perplexity_gap', 'stability_score']
        }

        save_gpu_features(features_dict, f"hsse_{name}")

        print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(features)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {features.shape}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {name}:")
        for i, name in enumerate(['Semantic', 'Stylometric', 'Perplexity Gap', 'Stability']):
            feature_values = features[:, i]
            print(f"   {name}: mean={np.mean(feature_values):.4f}, "
                  f"std={np.std(feature_values):.4f}, "
                  f"min={np.min(feature_values):.4f}, "
                  f"max={np.max(feature_values):.4f}")


if __name__ == "__main__":
    extract_hsse_features()